---
title: "HW23"
authors: "Daniel Oliner, Musab Alquwaee, Jacob McGill"
output: md_document
date: "2024-03-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
dengue = read.csv("C://Users/jacob/Downloads/dengue.csv")
greenhouses <- read.csv("C://Users/jacob/Downloads/greenbuildings.csv")
housing = read.csv('C://Users/jacob/Downloads/CAhousing.csv', header=TRUE)
stadia_key = Sys.getenv("stadia_key")
library(rpart)
library(randomForest)
library(gbm)
library(rpart.plot)
library(Metrics)
library(rsample)
library(pdp)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(rsample)
library(mosaic)
library(caret)
library(glmnet)
library(ggmap)
library(patchwork)
library(ggdensity)
library(lubridate)
library(modelr)
```


# Question 1

## 1

You cannot just regress "Crime" on "Police" to understand the impact of more cops in the street on crime due to the fact that cities with more crime are likely to have more police officers.Because this unobserved 3rd variable exists, regressing crime on police will not produce an accurate measure of the effect of the number of police officers on the crime rate.

## 2

To isolate the effect of the number of police officers on crime, the researchers regressed crime on a dummy variable indicating when Washington DC was on a "high" terror alert. Since this alert resulted in a heightened police presence but is not influenced by street crime, it could act as instrumental variable to estimate the causal effect of heightened police presence on crime. As can be seen in the table, days on High Alert saw on average 7.136 less daily crimes, an effect that is statistically significant at the 5% level.

## 3

The authors controlled for Metro ridership to control for if tourists were less likely to visit Washington DC or be out in the city when it was high terror alert. The control is attempting to capture any reduction in daily crime that would have resulted from less tourists being in the city who could be potential victims.

## 4

The model estimates the effect of an interaction between being on high alert and in Washington DC District 1 (the district containing the National Mall), an interaction between high alert and being in the other police districts, and the log of midday ridership on the DC Metro on daily crime rates. It is estimating the difference between the decrease in daily crime rates in the District containing the National Mall (which sees a greater increases in police presence) compared to other districts, controlling for people travelling around the city. As can be seen, District 1 has a decreases of -2.62 average daily crime rates when high alert is declared, while no statistically significant effect is estimated in the other districts.


# Question 2

```{r, message=FALSE, echo=FALSE}
# Exclude rows with missing values
dengue_clean <- na.exclude(dengue)

# Convert 'city' and 'season' to factors
dengue_clean$city <- as.factor(dengue_clean$city)
dengue_clean$season <- as.factor(dengue_clean$season)

set.seed(123) # Seed for reproducibility
split <- initial_split(dengue_clean, prop = 0.9)
train <- training(split)
test <- testing(split)

```

```{r, message=FALSE, echo=FALSE}
# Fit the initial CART model
cart_fit <- rpart(total_cases ~ ., data = train, 
                  control = rpart.control(cp = 0.002, minsplit = 20))

# Plot the un-pruned CART model
rpart.plot(cart_fit, digits = 3, type = 4, extra = 1)

# Prune the CART model
prune_func <- function(tree) {
  fit_details <- as.data.frame(tree$cptable)
  optimal_cp <- with(fit_details, max(CP[xerror - xstd <= min(xerror)]))
  prune(tree, cp = optimal_cp)
}
cart_pruned <- prune_func(cart_fit)

# Calculate RMSE for the pruned CART model
pred_cart <- predict(cart_pruned, test)
rmse_cart <- sqrt(mean((pred_cart - test$total_cases)^2))
```
```{r, message=FALSE, echo=FALSE}
# Fit the Random Forest model
rf_fit <- randomForest(total_cases ~ ., data = train, ntree = 200)

# Calculate RMSE for the Random Forest model
pred_rf <- predict(rf_fit, test)
rmse_rf <- sqrt(mean((pred_rf - test$total_cases)^2))
```

```{r, message=FALSE, echo=FALSE}
# Fit the GBM model
gbm_fit <- gbm(total_cases ~ ., data = train, 
               distribution = "gaussian", n.trees = 300, 
               interaction.depth = 3, shrinkage = 0.1, 
               cv.folds = 5, n.cores = 1)

# Determine the optimal number of trees
opt_trees <- gbm.perf(gbm_fit, method = "cv")

# Calculate RMSE for the GBM model
pred_gbm <- predict(gbm_fit, test, n.trees = opt_trees)
rmse_gbm <- sqrt(mean((pred_gbm - test$total_cases)^2))
```

The GBM model's performance plot reveals how it learns over time, with the training error steadily decreasing, reflecting a good fit on training data. The validation error's dip and subsequent plateau suggest an optimal number of trees at the dashed line, beyond which the model may start to overfit.

```{r, message=FALSE, echo=FALSE}
cat("RMSE for Pruned CART Model: ", rmse_cart, "\n")
cat("RMSE for Random Forest Model: ", rmse_rf, "\n")
cat("RMSE for GBM Model: ", rmse_gbm, "\n")
```

The Random Forest model outperforms its counterparts with an RMSE of 23.67, making it the most accurate for predicting dengue cases in this dataset, while the CART and GBM models show higher errors, at 28.40 and 25.93 respectively.

```{r, message=FALSE, echo=FALSE}
if (rmse_rf == min(c(rmse_cart, rmse_rf, rmse_gbm))) {
  # PDP for specific_humidity
  pdp_humidity <- partial(rf_fit, pred.var = "specific_humidity", train = train, grid.resolution = 10)
  plot(pdp_humidity, main = "Partial Dependence Plot for Specific Humidity", xlab = "Specific Humidity", ylab = "Predicted Dengue Cases")
  
  # PDP for precipitation_amt
  pdp_precip <- partial(rf_fit, pred.var = "precipitation_amt", train = train, grid.resolution = 10)
  plot(pdp_precip, main = "Partial Dependence Plot for Precipitation Amount", xlab = "Precipitation Amount", ylab = "Predicted Dengue Cases")
  
  # PDP for tdtr_k
  pdp_tdtr <- partial(rf_fit, pred.var = "tdtr_k", train = train, grid.resolution = 10)
  plot(pdp_tdtr, main = "Partial Dependence Plot for Diurnal Temperature Range", xlab = "Temperature Range", ylab = "Predicted Dengue Cases")
} else {
  cat("The Random Forest model did not perform best. Adjust the code to plot PDPs for the best-performing model.")
}
```

The Partial Dependence Plots suggest that higher specific humidity may increase dengue cases, while diurnal temperature range shows a possible decrease in cases with greater fluctuations. Precipitation amount, however, doesn't show a clear impact, indicating that other factors might be at play or that its relationship to dengue cases isn't straightforward. These insights can help focus preventive measures on more impactful environmental factors.

# Question 3

# Introduction 

Using a comprehensive dataset on 7,894 commercial rental propertes from across the United States, we will first build the best predicitve model possible for revenue per square foot per calendar year. Using this model we will quantify the average change in rental income per square foot associated with green certification, combining both LEED and EnergyStar as a single "green certified" category for simplicity.

# Data Preparation 

Before building the model, we begin by preparing the dataset. Three key adjustments: Defining the revenue per square foot variable as the product of Rent and Leasing Rate, defining a single "green certified" variable that is equal to one if the building is either LEED or EnergyStar certified, and splitting the data into training and testing sets. 

```{r, include=FALSE}

# Create revenue per square foot per year variable
greenhouses$revenue_psqft <- greenhouses$Rent * greenhouses$leasing_rate

# Create a single green certification variable that accounts for both LEED and EnergyStar
greenhouses$green_certified <- ifelse(greenhouses$LEED == 1 | greenhouses$Energystar == 1, 1, 0)
greenhouses <- na.omit(greenhouses)
set.seed(123)

split <- initial_split(greenhouses, prop = 0.8)
train_set <- training(split)
test_set <- testing(split)
```

# Model Construction 

In the model construction phase, we established two models: a comprehensive baseline model using linear regression and a random forest model both aimed at predicting the revenue per square foot (revenue_psqft). The baseline linear model serves as a reference point against which we can evaluate the relative performance of the random forest model. 

For the baseline linear regression model, we included relevant predictors and introduced an interaction term between market rent and size. In contrast, the random forest model was trained with a subset of predictors selected based on their importance in predicting revenue per square foot. Our random forest model construction utilized variable importance metrics to identify influential predictors.

By comparing the performance metrics of the random forest model against the baseline linear regression model, we can assess the effectiveness of using a more sophisticated method for predicting rental revenue per square foot.


```{r, echo=FALSE}
lm_model <- lm(revenue_psqft ~ size + size * City_Market_Rent + age + 
                         leasing_rate + class_a + class_b + amenities + 
                         total_dd_07 + Gas_Costs + Electricity_Costs + green_certified,data  = train_set)
summary(lm_model)


rf_model <- randomForest(revenue_psqft ~ size +  age + leasing_rate + stories + renovated + class_a + class_b + amenities +
                         City_Market_Rent + Gas_Costs + Electricity_Costs + green_certified, 
                         data = train_set, importance = TRUE)

var_importance <- importance(rf_model)
print(rf_model)

```

# Model Evaluation

```{r, echo=FALSE}
lm_predictions <- predict(lm_model, newdata = test_set)
rf_predictions <- predict(rf_model, newdata = test_set)

lm_rmse <- sqrt(mean((lm_predictions - test_set$revenue_psqft)^2))
rf_rmse <- sqrt(mean((rf_predictions - test_set$revenue_psqft)^2))

cat("Linear Model RMSE:", lm_rmse, "\n")
cat("Random Forest RMSE:", rf_rmse, "\n")

```
In the model evaluation phase, we compare the predictive performance of the baseline linear regression model and the random forest model on the testing dataset. The random forest model achieved a lower RMSE than the linear regression model. This indicates that the average deviation between the predictions of the actual revenue per square foot is significantly lower for the random forest model, thus the random forest model displays superior predictive accuracy compared to the baseline model. The random forest model also explains about 86% of the variation in revenue per square foot, a signficant improvement over the R-Squared value for the linear model, further evidencing it's enhanced performance. 

By leveraging a set of decision trees, the random forest model can better adapt to the complexities inherent in the data, resulting in more accurate predictions of rental revenue per square foot. Therefore, the random forest model emerges as the preferred choice for its predictive accuracy and we will use it to determine the average change in rental income associated with green certification. 

# Quantifying the Average Change in Revenue Per Square Foot Associated With Green Certification

```{r, echo=FALSE}

# Predict revenue per square foot for the testing set using the random forest model
test_set$predicted_revenue <- predict(rf_model, newdata = test_set)

# Calculate the average predicted for green-certified buildings
avg_revenue_green <- mean(test_set$predicted_revenue[test_set$green_certified == 1])

# Calculate the average predicted for non-green-certified buildings
avg_revenue_non_green <- mean(test_set$predicted_revenue[test_set$green_certified == 0])

average_change <- avg_revenue_green - avg_revenue_non_green

# Print the results
cat("Average Change in Rental Income Per Square Foot Associated With Green Certification:", average_change, "\n")

partialPlot(rf_model, train_set, x.var = "green_certified")
```

# Conclusion

Our objective was to predict revenue per square foot for commercial rental properties and to quantify the financial impact of green certification. Data preprocessing was conducted, and the dataset was split into training and testing subsets. We developed predictive models using linear regression and random forest techniques, with the random forest model outperforming its linear regression counterpart in terms of predictive accuracy. We used the random forest model to determine that a building with a green certification is estimated to generate about $300 in additional annual revenue per square foot. However, the partial dependence plot revealed a positive but modest relationship when the effects of other variables are controlled, indicating that green certification has a positive, albeit smaller than initially estimated, isolated impact on predicted annual revenue per square foot.  This nuanced finding suggests that while green certification is beneficial, the magnitude of its isolated effect on revenue per square foot is not as substantial when considered in the context of other contributing factors.

# Question 4
```{r, include = FALSE}
register_stadiamaps(stadia_key, write = FALSE)
Cal = c(left = -124.5, bottom = 32, right = -112, top = 42)
cal_test = get_stadiamap(Cal, zoom = 5, maptype = "alidade_smooth")

#Modify housing data to plot median house prices
house_val = housing %>%
  mutate(lon = longitude, lat = latitude) %>%
  select(lon, lat, medianHouseValue)
```
```{r, include = FALSE}
#Now lets predict housing prices using trees. Going over the notes, it looks like we may want to use either random forests or boosted trees, although right now I'm leaning towards random forest since it requires less tuning. Below is a good start, where we have the models established. Next steps would be standardizing by households and including an estimate for the overall out of sample accuracy of the proposed model. May also want to consider looking at log values instead too.
house_mod = housing %>%
  mutate(stand_rooms = totalRooms/households,
         stand_bedrooms = totalBedrooms/households)
house_split =  initial_split(house_mod, prop=0.8)
house_train = training(house_split)
house_test  = testing(house_split)
house.forest = randomForest(medianHouseValue ~ latitude + longitude + housingMedianAge + stand_rooms + stand_bedrooms +stand_rooms + population + households + medianIncome, data=house_train, importance = TRUE)
```
To predict median House Value with the features provided, we decided to use a random forest model, specifically using the randomForest function in R. Both totalRooms and totalBedrooms listed the number of bedrooms and rooms for the census track, which could be problematic. To adjust this, Total Rooms and Total Bedrooms were standardized by census tract population, dividing both values by the population of their census tract. The resulting variables were stand_bedrooms and stand_rooms, which measure bedrooms and rooms by track populations. Apart from these modifications, we used all other columns in the data as is. Since the number of variables was small (only 8), we did not use PCA to simplify the data before finding the tree. Further, as we used the random forest model, cross validation was not necessary.After running the randomForest model on the training data, we tested its out of sample accuracy to estimate the following RMSE as a measure of out of sample accuracy.
```{r, include = TRUE}
modelr::rmse(house.forest, house_test)
```
Next we are going to plot the actual median house values, the predicted median house values and the residuals of the model's predictions
```{r, include = FALSE}

house_mod = house_mod %>%
  mutate(house_pred = predict(house.forest, house_mod),
           housing_resid = medianHouseValue - house_pred)
```
```{r, include = TRUE}

house_plot = ggplot(house_val) + 
  geom_point(aes(x=lon, y=lat, color=medianHouseValue)) + 
  scale_color_continuous(type = "viridis")
ggmap(cal_test) + geom_point(data = house_val, aes(x=lon, y=lat, color=medianHouseValue))  + 
  scale_color_continuous(type = "viridis")

house_predict = house_mod %>%
  mutate(lon = longitude, lat = latitude) %>%
  select(lon, lat, house_pred)
ggmap(cal_test) + geom_point(data = house_predict, aes(x=lon, y=lat, color=house_pred))  + 
  scale_color_continuous(type = "viridis")

house_res = house_mod %>%
  mutate(lon = longitude, lat = latitude) %>%
  select(lon, lat, housing_resid)
ggmap(cal_test) + geom_point(data = house_res, aes(x=lon, y=lat, color=housing_resid))  + 
  scale_color_continuous(type = "viridis")
```
As can be seen in this 3 graphs, the model appears to do a decent job of predicting the median value of houses in a census tract. The model appears to slightly underestimate median house value compared to the actual median house value, but makes relatively close predictions on average, as can be seen with the residual dots that are mostly around dark green, light blue color. 
